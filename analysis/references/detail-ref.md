# Gemini Robotics Paper - Detailed References

This document contains the structured references from the Gemini Robotics paper, organized by year with links and citation information.

Format: [year][venue][paper link][github repo link] **title**, first author

## 2006

[2006][book][NO LINK][NO CODE] **Planning algorithms**, Steven M LaValle

## 2011

[2011][ISO][\[pub\]](https://books.google.com/books?id=BaF-AQAACAAJ)[NO CODE] **ISO 10218: Robots and Robotic Devices : Safety Requirements for Industrial Robots**, International Organization for Standardization

## 2012

[2012][ANSI/RIA][NO LINK][NO CODE] **ANSI/RIA R15.06-2012: Safety requirements for industrial robots and robot systems**, Robotic Industries Association (RIA)

## 2014

[2014][conference][NO LINK][NO CODE] **ISO 13482 - the new safety standard for personal care robots**, Theo Jacobs

## 2015

[2015][CVPR][NO LINK][NO CODE] **SUN RGB-D: A RGB-D scene understanding benchmark suite**, Shuran Song

## 2016

[2016][book chapter][NO LINK][NO CODE] **Force control**, Luigi Villani

## 2018

[2018][software][\[pub\]](http://github.com/google/jax)[\[code\]](http://github.com/google/jax) **JAX: composable transformations of Python+NumPy programs**, James Bradbury

## 2019

[2019][ECC][NO LINK][NO CODE] **Control barrier functions: Theory and applications**, Aaron D Ames

[2019][conference][NO LINK][NO CODE] **Model cards for model reporting**, Margaret Mitchell

## 2020

[2020][CVPR][\[doi\]](10.1109/CVPR42600.2020.00014)[NO CODE] **Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image**, Yinyu Nie

[2020][IEEE RA Letters][NO LINK][NO CODE] **Benchmarking protocols for evaluating small parts robotic assembly systems**, Kenneth Kimble

## 2021

[2021][ICML][\[pub\]](http://proceedings.mlr.press/v139/radford21a.html)[NO CODE] **Learning transferable visual models from natural language supervision**, Alec Radford

[2021][WACV][NO LINK][NO CODE] **The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain**, Francesco Ragusa

[2021][TPAMI][NO LINK][NO CODE] **In the eye of the beholder: Gaze and actions in first person video**, Yin Li

[2021][blog][\[pub\]](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)[NO CODE] **Introducing Pathways: A next-generation AI architecture**, Jeff Dean

[2021][CVPR][\[doi\]](10.1109/CVPR46437.2021.00875)[NO CODE] **Holistic 3D scene understanding from a single image with implicit representation**, Jiayi Zhang

## 2022

[2022][arXiv][\[arxiv\]](https://arxiv.org/abs/2204.01691)[NO CODE] **Do As I Can, Not As I Say: Grounding Language in Robotic Affordances**, Michael Ahn

[2022][arXiv][\[arxiv\]](https://arxiv.org/abs/2212.08073)[NO CODE] **Constitutional AI: Harmlessness from AI feedback**, Yuntao Bai

[2022][dataset][NO LINK][NO CODE] **Meccano: A multimodal egocentric dataset for humans behavior understanding in the industrial-like domain**, Francesco Ragusa

[2022][WACV][\[doi\]](10.1109/WACV51458.2022.00120)[NO CODE] **ImVoxelNet: Image to voxels projection for monocular and multi-view 3d object detection**, Danila Rukhovich

[2022][NIPS][NO LINK][NO CODE] **Chain-of-Thought prompting elicits reasoning in large language models**, Jason Wei

[2022][blog][NO LINK][NO CODE] **How our principles helped define AlphaFold's release**, Koray Kavukcuoglu

## 2023

[2023][workshop][\[pub\]](https://openreview.net/forum?id=T8AiZj1QdN)[NO CODE] **How to prompt your robot: A promptbook for manipulation skills with code as policies**, Montserrat Gonzalez Arenas

[2023][CoRL][\[pub\]](https://proceedings.mlr.press/v229/zitkovich23a.html)[NO CODE] **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**, Anthony Brohan

[2023][arXiv][\[arxiv\]](https://arxiv.org/abs/2311.01977)[NO CODE] **RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches**, Jiayuan Gu

[2023][ICRA][NO LINK][NO CODE] **Code as policies: Language model programs for embodied control**, Jacky Liang

[2023][arXiv][\[arxiv\]](https://arxiv.org/abs/2312.11805)[\[pub\]](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) **Gemini: a family of highly capable multimodal models**, Gemini-Team

[2023][CVPR][NO LINK][NO CODE] **Paco: Parts and attributes of common objects**, Vignesh Ramanathan

[2023][arXiv][\[arxiv\]](https://arxiv.org/abs/2310.13798)[NO CODE] **Specific versus general principles for constitutional AI**, Sandipan Kundu

[2023][RSS][\[doi\]](10.15607/RSS.2023.XIX.016)[NO CODE] **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**, Tony Z. Zhao

[2023][ICCV][NO LINK][NO CODE] **HoloAssist: an egocentric human interaction dataset for interactive AI assistants in the real world**, Xin Wang

[2023][arXiv][\[arxiv\]](https://arxiv.org/abs/2306.17582)[NO CODE] **ChatGPT for Robotics: Design principles and model abilities**, Sai Vemprala

## 2024

[2024][arXiv][NO LINK][NO CODE] **AutoRT: Embodied foundation models for large scale orchestration of robotic agents**, Michael Ahn

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2407.07726)[NO CODE] **PaliGemma: A versatile 3B VLM for transfer**, Lucas Beyer

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2410.24164)[NO CODE] **π0: A vision-language-action flow model for general robot control**, Kevin Black

[2024][CVPR][NO LINK][NO CODE] **SpatialVLM: Endowing vision-language models with spatial reasoning capabilities**, Boyuan Chen

[2024][IJRR][NO LINK][NO CODE] **Diffusion policy: Visuomotor policy learning via action diffusion**, Cheng Chi

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2409.17146)[NO CODE] **Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models**, Matt Deitke

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2403.19578)[NO CODE] **Keypoint action tokens enable in-context imitation learning in robotics**, Norman Di Palo

[2024][NeurIPS][\[pub\]](https://openreview.net/forum?id=P5dEZeECGu)[NO CODE] **FlexCap: Describe anything in images in controllable detail**, Debidatta Dwibedi

[2024][ECCV][NO LINK][NO CODE] **BLINK: Multimodal large language models can see but not perceive**, Xingyu Fu

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2410.23262)[NO CODE] **Emma: End-to-end multimodal model for autonomous driving**, Jyh-Jing Hwang

[2024][FAT][NO LINK][NO CODE] **Collective constitutional AI: Aligning a language model with public input**, Saffron Huang

[2024][RA Letters][\[doi\]](10.1109/lra.2024.3410155)[\[pub\]](http://dx.doi.org/10.1109/LRA.2024.3410155) **Language models as zero-shot trajectory generators**, Teyun Kwon

[2024][ICRA][\[doi\]](10.1109/ICRA57147.2024.10611477)[NO CODE] **Open X-Embodiment: Robotic Learning Datasets and RT-X Models**, Abby O'Neill

[2024][NEISS][NO LINK][NO CODE] **National Electronic Injury Surveillance System - All Injury Program (NEISS-AIP)**, NEISS

[2024][ICRA][NO LINK][NO CODE] **Robotap: Tracking arbitrary points for few-shot visual imitation**, Mel Vecerik

[2024][IROS][\[pub\]](http://dblp.uni-trier.de/db/conf/iros/iros2024.html#VarleySJC0CDS24)[NO CODE] **Embodied AI with two arms: Zero-shot learning, safety and modularity**, Jake Varley

[2024][RSS][NO LINK][NO CODE] **Any-point trajectory modeling for policy learning**, Chuan Wen

[2024][dataset][\[pub\]](https://huggingface.co/datasets/xai-org/RealworldQA)[NO CODE] **RealworldQA: a dataset of real-world questions from the XAI-Bench suite**, XAI-org

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2406.10721)[NO CODE] **Robopoint: A vision-language model for spatial affordance prediction for robotics**, Wentao Yuan

[2024][CoRL][NO LINK][NO CODE] **Robotic control via embodied chain-of-thought reasoning**, Michał Zawalski

[2024][arXiv][\[arxiv\]](https://arxiv.org/abs/2405.02292)[NO CODE] **ALOHA 2: An enhanced low-cost hardware for bimanual teleoperation**, ALOHA 2 Team

[2024][dataset][\[pub\]](https://umi-data.github.io/)[\[code\]](https://umi-data.github.io/) **UMI-Data**, UMI-Data

## 2025

[2025][CoRL][\[pub\]](https://proceedings.mlr.press/v270/xu25b.html)[NO CODE] **Mobility VLA: Multimodal instruction navigation with long-context VLMs and topological graphs**, Hao-Tien Lewis Chiang

[2025][CoRL][\[pub\]](https://proceedings.mlr.press/v270/kim25c.html)[NO CODE] **OpenVLA: An open-source Vision-Language-Action model**, Moo Jin Kim

[2025][arXiv][\[arxiv\]](https://arxiv.org/abs/2503.01238)[NO CODE] **A taxonomy for evaluating generalist robot policies**, Jensen Gao

[2025][Google report][\[pub\]](https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf)[NO CODE] **Responsible AI progress report**, Google

[2025][arXiv][\[arxiv\]](https://arxiv.org/abs/2502.05485)[NO CODE] **Hamster: Hierarchical action models for open-world robot manipulation**, Yi Li

[2025][arXiv][\[pub\]](https://sites.google.com/view/proc4gem)[NO CODE] **Proc4Gem: Foundation models for physical agency through procedural generation**, Yixin Lin

[2025][CoRL][\[pub\]](https://proceedings.mlr.press/v270/zhao25b.html)[NO CODE] **ALOHA Unleashed: A Simple Recipe for Robot Dexterity**, Tony Z. Zhao

[2025][arXiv][\[arxiv\]](https://arxiv.org/abs/2503.08663)[NO CODE] **Generating Robot Constitutions & Benchmarks for Semantic Safety**, Pierre Sermanet

[2025][arXiv][\[arxiv\]](http://arxiv.org/abs/2503.10706)[NO CODE] **SciFi-Bench: How Would AI-Powered Robots Behave in Science Fiction Literature?**, Pierre Sermanet

---

## Summary

Total references: 79
- arXiv papers: 20
- Conference papers: 35
- Journal articles: 8
- Books/Book chapters: 3
- Datasets: 3
- Standards/Reports: 4
- Blog posts: 2
- Software/Tools: 4

### Year Distribution
- 2006: 1
- 2011: 1
- 2012: 1
- 2014: 1
- 2015: 1
- 2016: 1
- 2018: 1
- 2019: 2
- 2020: 3
- 2021: 6
- 2022: 7
- 2023: 10
- 2024: 27
- 2025: 17